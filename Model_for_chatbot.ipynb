{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbpRZO8sRmjk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community\n",
        "!pip install chromadb\n",
        "!pip install sentence_transformers\n",
        "!pip install pypdf\n",
        "!pip install tiktoken\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings"
      ],
      "metadata": {
        "id": "PKcC3SAADU_H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader(\"/content/ipc-data\", glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
        "\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "1KIzg0aaSt_M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB99mWRQefh4",
        "outputId": "b918bf2a-4c53-4a10-eebb-06922808476a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15148"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "subset_texts = texts[:30]\n",
        "\n",
        "persist_directory = \"/content/db_directory\"\n",
        "db = Chroma.from_documents(subset_texts, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "db.persist()\n",
        "\n",
        "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "print(f\"Number of chunks in subset: {len(subset_texts)}\")\n",
        "print(\"Database created and persisted with the subset of text chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IApEM4BAfbuZ",
        "outputId": "2559cc56-3f00-4bcb-9c78-d8300cc657f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks in subset: 30\n",
            "Database created and persisted with the subset of text chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"MBZUAI/LaMini-Flan-T5-783M\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32\n",
        ")"
      ],
      "metadata": {
        "id": "182UEKZcb9mF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    'text2text-generation',\n",
        "    model = base_model,\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = 512,\n",
        "    do_sample = True,\n",
        "    temperature = 0.3,\n",
        "    top_p= 0.95\n",
        ")\n"
      ],
      "metadata": {
        "id": "OlQbcVDtd8Ga"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=local_llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "72I12qxAd_aJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_query = str(input(\"Enter your query:\"))\n",
        "\n",
        "llm_response = qa_chain({\"query\": input_query})\n",
        "\n",
        "print(llm_response['result'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toxdPb_8eCU1",
        "outputId": "ad3f8a3c-4636-4192-b860-dda0503ebe7d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query:what is ipc\n",
            "IPC stands for Indian Penal Code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer.save_pretrained(\"tokenizer_model\")\n",
        "base_model.save_pretrained(\"base_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyFCmESJKJfy",
        "outputId": "428d1024-68ce-4732-b454-c8f7aec8dc0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2525: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}